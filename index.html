

<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <style>
        a {
            text-decoration: none; /* Î∞ëÏ§Ñ ÏóÜÏï†Í∏∞ */
            color: darkblue; /* ÎßÅÌÅ¨ ÏÉâÏÉÅ ÏÑ§Ï†ï */
        }
        a:hover {
            /*text-decoration: underline; /* ÎßàÏö∞Ïä§ Ïò§Î≤Ñ Ïãú Î∞ëÏ§Ñ Ï∂îÍ∞Ä (ÏÑ†ÌÉù ÏÇ¨Ìï≠) */
	    color: blue;
        }
    </style>

  <title>Sooyoung Kim</title>
  <link rel="icon" type="image/x-icon" href="images/icon_img.jpg">

  <meta name="author" content="Sooyoung Kim">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin=anonymous>
  <link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin=anonymous>


</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px; style="line-height:200%">
      <td style="padding:0px">
	<div class="docs-section" id="about">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;text-align:justify;">
              <p style="text-align: center">
                <strong><font size="6"><name>Sooyoung Kim</name></font></strong>
              </p>
	      <p>I am an incoming CS Ph.D. student at Rutgers University, advised by <a href="https://scholar.google.com/citations?hl=en&user=8MQT8skAAAAJ&view_op=list_works&sortby=pubdate">Vladimir Pavlovic</a>. </p>
              <p>I received M.S. in <a href="https://bcs.snu.ac.kr/">Brain and Cognitive Sciences</a> at <a href="https://en.snu.ac.kr/index.html">Seoul National University</a>, under the supervision of <a href="https://scholar.google.com/citations?user=fHSAoOoAAAAJ&hl=en&oi=ao">Prof.Jiook Cha</a>. Since my Master's program, I have been fortunate to be advised by <a href="https://iacs.stonybrook.edu/people/_affiliates/shinjae-yoo.php">Prof.Shinjae Yoo</a> and <a href="https://ywlincq.github.io/">Prof.Yuewei Lin</a> at <a href="https://www.bnl.gov/world/">Brookhaven National Laboratory (BNL)</a>. Previously, I received B.S. in <a href="https://cms.ewha.ac.kr/user/indexMain.action?siteId=cseeng">Computer Science and Engineering</a> at <a href="https://www.ewha.ac.kr/ewhaen/index.do">Ewha Womans University</a>. </p>

	      <br>
	      <p>My research interests lie in <b>Computer Vision</b>, <b>Generative AI</b>, and <b>Human-AI</b>. I am interested in understanding and generating <b>images, videos, and 3D/4D content</b>, particularly applications to narrative-driven multimedia like movies. I aim to develop deep learning models that are (i) controllable for human interaction, (ii) visually coherent, and capable of (iii) perceiving the visual, semantic, and physical representation of the real world. </p>
	      <div class="social-icons">
	        <a style="margin: 0 7px 0 0" href="https://github.com/Sooyyoungg">
	          <i class="fab fa-github" style="font-size:1.5rem"></i>
	        </a>
	        <a style="margin: 0 7px 0 0" href="https://www.linkedin.com/in/swimming-whale/">
	          <i class="fab fa-linkedin" style="font-size:1.5rem"></i>
	        </a>
		<a style="margin: 0 7px 0 0" href="https://scholar.google.com/citations?hl=en&user=OR86qUIAAAAJ">
	          <i class="ai ai-google-scholar" style="font-size:1.5rem"></i>
	        </a>
		<a style="margin: 0 7px 0 0" href="data/CV_Sooyoung_Kim_.pdf">
	          <i class="ai ai-cv" style="font-size:1.6rem;"></i>
	        </a>
		&nbsp<a href="mailto:rlatndud0513@snu.ac.kr">rlatndud0513@snu.ac.kr</a>
	      </div>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/profile.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/profile.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
	</div>



	      
        <!-- ========== News ========== -->
	<div class="docs-section" id="news">
	  <strong><font size="5"><h5>Newsüî•</h5></font></strong>
	  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	        <tr>
	            <td width="80" style="color:#fc83a6;font-weight: bold;vertical-align: top;">Feb 2025</td>
	            <td>Invited as a reviewer from journal <b>Pattern Recognition</b> and <b>IEEE Computational Intelligence Magazine</b>.</td>
	        </tr>
		<tr>
	            <td width="80" style="color:#fc83a6;font-weight: bold;vertical-align: top;">Dec 2024</td>
	            <td>Our paper, <a href="https://arxiv.org/abs/2412.05296">Revisiting Your Memory</a>, is accepted to the <b>AAAI 2025 Workshop on AI for Music</b>.</td>
	        </tr>
		<tr>
	            <td width="80" style="color:#fc83a6;font-weight: bold;vertical-align: top;">Oct 2024</td>
	            <td>Our work, "The Recollection of Your Most Cherished Experiences utilizing AI and Neural Signals", will be presented at the Tech to Art Platform (TAP) International Conference Prequel, the ART DIFFUSION.</td>
	        </tr>
	        <tr>
	            <td width="80" style="color:#fc83a6;font-weight: bold;vertical-align: top;">Oct 2024</td>
	            <td>Our team won <b>the GRAND prize</b> üèÖ at the AI & Art Hackathon.</td>
	        </tr>
		<tr>
	            <td width="80" style="color:#fc83a6;font-weight: bold;vertical-align: top;">Oct 2024</td>
	            <td>Started working as an AI researcher with <a href="https://www.planningo.io/">Planningo</a> via AI research Partnership.</td>
	        </tr>
		<tr>
	            <td width="80" style="color:#fc83a6;font-weight: bold;vertical-align: top;">Sep 2024</td>
	            <td>Selected to participate in the AI x Art Hackathon hosted by the AI Art Research Center in Seoul, Korea.</td>
	        </tr>
		<tr>
	            <td width="80" style="color:#fc83a6;font-weight: bold;vertical-align: top;">Aug 2024</td>
	            <td>Invited for a talk on the brain decoding project at Seoul National University.</td>
	        </tr>
		<tr>
	            <td width="80" style="color:#fc83a6;font-weight: bold;vertical-align: top;">Dec 2023</td>
	            <td>Our <a href="https://aesfa-nst.github.io/AesFA/">AesFA</a> paper got accepted to <b>AAAI 2024</b>.</td>
	        </tr>
		<tr>
	            <td width="80" style="color:#fc83a6;font-weight: bold;vertical-align: top;">Aug 2023</td>
	            <td>I received M.S. at Seoul National University. I will continue doing research at <a href="https://www.connectomelab.com/">ConnectomeLab</a>.</td>
	        </tr>
	    </table>
	</div>

	      
	<!-- ========== Research In Progress ========== -->
	<div class="docs-section" id="research_in_progress">
	<font size="5"><h5><strong>Research In Progress</strong> <br><font size="2">(* denotes equal contribution)</font></h5></font>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

	  <!--  Planningo  -->
	      <tr>
		<td style="padding:2%;width:25%;vertical-align:middle">
		  <img src='images/planningo.jpg' width="100%" height="auto">
		</td>
		<td style="padding:2%;width:75%;vertical-align:middle">
		  <span style="font-size: 18px"><strong><papertitle>An Instance-Adaptive Photorealistic Style Optimization for Relightful Image Harmonization‚Äã</papertitle></strong></span>
		  <br>
		  <a href="https://www.planningo.io/">Planningo</a>: AI-startup transforming the landscape of advertising photography and commercial videography
		  <div style="line-height:60%; font size=1"><br></div>
		  <font size="2">
		  My goal is to develop image harmonization models for <b>visual coherence</b> in commercial photography service <b><a href="https://www.photio.io/">Photio</a></b>. As there is an incongruence between AI-generated backgrounds and original advertising photography / commercial videography during synthesis, we fine-tuned the synthesis process by matching the lighting conditions of AI-generated backgrounds with those present in original commercial photography. This approach aims to enhance the visual coherence of images, ultimately benefiting the commercial AI photography industry.
		  </font>
		</td>
	      </tr> 
		
	  <!--  Brain decoding  -->
	      <tr>
		<td style="padding:2%;width:25%;vertical-align:middle">
		  <img src='images/brain_decoding.jpg' width="100%" height="auto">
		</td>
		<td style="padding:2%;width:75%;vertical-align:middle">
		  <span style="font-size: 18px"><strong><papertitle>Attention Guidance Enables A Composable Brain-To-Text Decoding</papertitle></strong></span>
		  <div style="line-height:20%; font size=1"><br></div>
		  <u><strong><span style="font-size: 16px">Sooyoung Kim*</span></strong></u>, Joonwoo Kwon*, Mincheol Park*, Jungwoo Seo, Won Woo Ro, Shinjae Yoo, Suhyun Kim, Yuewei Lin, Jiook Cha
		  <br>
		  <div style="line-height:60%; font size=1"><br></div>
		  <font size="2">
		  We present a novel brain-to-text/image decoding framework that enables composable prompt modulation using brain signals. The two-stream hypothesis is a well-known theory in neuroscience that explains how the brain processes visual information through two pathways: the ventral pathway, which identifies "what" an object is (shape, color, and identity), and the dorsal pathway, which determines "where" the object is (its position and motion in space). Inspired by this, we map these pathways to corresponding image features. Our unified brain encoding module guides object recognition and positioning using diffusion models, with brain-induced attention guidance modifying LDM cross-attention layers during inference, without additional fine-tuning.		  </font>
		</td>
	      </tr>

	  <!--  AesPHA  -->
	      <tr>
		<td style="padding:2%;width:25%;vertical-align:middle">
		  <img src='images/aespha.jpg' width="100%" height="auto">
		</td>
		<td style="padding:2%;width:75%;vertical-align:middle">
		  <span style="font-size: 18px"><strong><papertitle>An Aesthetically Enhanced Brushstrokes Parameterization for Neural Style Transfer</papertitle></strong></span>
		  <div style="line-height:20%; font size=1"><br></div>
		  Joonwoo Kwon*, <u><strong><span style="font-size: 16px">Sooyoung Kim*</span></strong></u>, Seungjun Lee, Shinjae Yoo, Yuewei Lin, Jiook Cha
		  <br>
		  <div style="line-height:60%; font size=1"><br></div>
		  <font size="2">
		  We suggest the neural style transfer framework by leveraging the Navier-Stokes equations for physical representations of parameterized brushstrokes.
		  </font>
		</td>
	      </tr>
		
            </tbody>
	  </table>
	</div>
	      
	<!-- ========== Publications ========== -->
	<div class="docs-section" id="publications">
	<strong><font size="5"><h5>Publications</h5></font></strong>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

	<!--  Music Style Transfer  -->
	      <tr>
		<td style="padding:2%;width:25%;vertical-align:middle">
		  <img src='images/music.png' width="100%" height="auto">
		</td>
		<td style="padding:2%;width:75%;vertical-align:middle">
		  <span style="font-size: 18px"><strong><papertitle>A Training-Free Approach for Music Style Transfer with Latent Diffusion Models</papertitle></strong></span>
		  <div style="line-height:20%; font size=1"><br></div>
		  <u><strong><span style="font-size: 16px">Sooyoung Kim*</span></strong></u>, Joonwoo Kwon*, Heehwan Wang*, Shinjae Yoo, Yuewei Lin, Jiook Cha
		  <br>
		  <em> Preprint.</em> <br>
		  <a href="https://arxiv.org/abs/2411.15913">ArXiv</a>
		</td>
	      </tr> 
		
	<!--  MRI Synthesis  -->
	      <tr>
		<td style="padding:2%;width:25%;vertical-align:middle">
		  <img src='images/mri_synthesis.jpg' width="100%" height="auto">
		</td>
		<td style="padding:2%;width:75%;vertical-align:middle">
		  <span style="font-size: 18px"><strong><papertitle>Macro2Micro: A Rapid and Precise Cross-modal Magnetic Resonance Imaging Synthesis using Multi-scale Structural Brain Similarity</papertitle></strong></span>
		  <div style="line-height:20%; font size=1"><br></div>
		  <u><strong><span style="font-size: 16px">Sooyoung Kim*</span></strong></u>, Joonwoo Kwon*, Junbeom Kwon, Sangyoon Bae, Yuewei Lin, Shinjae Yoo, Jiook Cha
		  <br>
		  <em> Preprint </em> <br>
		  <a href="https://arxiv.org/abs/2412.11277">ArXiv</a>
		  <!--  /
		  <a href="https://github.com/Sooyyoungg/">Code</a> -->
		</td>
	      </tr> 
		
	  <!--  Affect  -->
	      <tr>
		<td style="padding:2%;width:25%;vertical-align:middle">
		  <img src='images/affect_eeg.png' width="100%" height="auto">
		</td>
		<td style="padding:2%;width:75%;vertical-align:middle">
		  <span style="font-size: 18px"><strong><papertitle>Revisiting Your Memory: Reconstruction of Affect-Contextualized Memory via EEG-guided Audiovisual Generation</papertitle></strong></span>
		  <div style="line-height:20%; font size=1"><br></div>
		  Joonwoo Kwon*, Heehwan Wang*, Jinwoo Yi*, <u><strong><span style="font-size: 16px">Sooyoung Kim*</span></strong></u>, Shinjae Yoo, Yuewei Lin, Jiook Cha
		  <br>
		  <em> AAAI 2025 Workshop on AI for Music</em> <br>
		  <a href="https://arxiv.org/abs/2412.05296">ArXiv</a>
		  <!--/
		  <a href="">Project page</a>
		  /
		  <a href="">Code</a>
		  -->
		</td>
	      </tr>
		
	  <!--  AesFA  -->
	      <tr>
		<td style="padding:2%;width:25%;vertical-align:middle">
		  <img src='images/aesfa.jpg' width="100%" height="auto">
		</td>
		<td style="padding:2%;width:75%;vertical-align:middle">
		  <span style="font-size: 18px"><strong><papertitle>AesFA: An Aesthetic Feature-Aware Arbitrary Neural Style Transfer</papertitle></strong></span>
		  <div style="line-height:20%; font size=1"><br></div>
		  Joonwoo Kwon*, <u><strong><span style="font-size: 16px">Sooyoung Kim*</span></strong></u>, Shinjae Yoo, Yuewei Lin, Jiook Cha
		  <br>
		  <em> AAAI, 2024 (23.75% acceptance rate; 2342/12100) </em> <br>
		  <a href="https://arxiv.org/abs/2312.05928">ArXiv</a>
		  /
		  <a href="https://aesfa-nst.github.io/AesFA/">Project page</a>
		  /
		  <a href="https://github.com/Sooyyoungg/AesFA">Code</a>
		</td>
	      </tr> 
		
	  <!--  Textbook  -->
	      <tr>
		<td style="padding:2%;width:25%;vertical-align:middle;text-align: center;">
		  <img src='images/textbook.jpg' width="90%" height="auto">
		</td>
		<td style="padding:2%;width:75%;vertical-align:middle">
		  <span style="font-size: 18px"><strong><papertitle>Designing Software Creation: Using UML Diagrams</papertitle></strong></span>
		  <div style="line-height:20%; font size=1"><br></div>
		  <!--  Participated as an assistant author. -->
		  <br>
		  Park, H.*, Kim, Y.*, Kim, Y.*, Ji, H.*, Oh, J., Nam, H., Lee, S., <u><b>Kim, S.</b></u>, Choi, S., Oh, Y., Huh, J., Song, D.
		  <em>Published textbook</em>, 2023
		  <br>
		  <a href="https://books.google.co.kr/books/about/%EC%86%8C%ED%94%84%ED%8A%B8%EC%9B%A8%EC%96%B4_%EC%B0%BD%EC%9D%98_%EC%84%A4%EA%B3%84_UML%EB%8B%A4%EC%9D%B4.html?id=EdXKEAAAQBAJ&redir_esc=y">Book</a>
		</td>
	      </tr> 

        </tbody></table>
	</div>


	<!-- ========== Projects ========== -->
	<div class="docs-section" id="research_in_progress">
	<strong><font size="5"><h5>Projects</h5></font></strong>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

	  <!--  AI x Art Hackathon  -->
	      <tr>
		<td style="padding:2%;width:25%;vertical-align:middle;text-align: center;">
		  <img src='images/art_diffusion.jpg' width="80%" height="auto">
		</td>
		<td style="padding:2%;width:75%;vertical-align:middle">
		  <span style="font-size: 18px"><strong><papertitle>The Recollection of Your Most Cherished Experiences utilizing AI and Neural Signals</papertitle></strong></span>
		  <br>The Grand Prize üèÖ at AI & Art Hackathon, AI Art Research Center, SNU
		  <div style="line-height:20%; font size=1"><br></div>
		  <u><strong><span style="font-size: 16px">Sooyoung Kim</span></strong></u>, Joonwoo Kwon, Heehwan Wang, Jinwoo Yi
		  <br>
		  <a href="data/ai_art.pdf">Presentation Slide</a>
		  <br>
		  <div style="line-height:60%; font size=1"><br></div>
		  <font size="2">
		  We proposed a multimodal AI framework for synthesizing personalized videography (video + music) utilizing generative AI and neural signals (EEG). In this project, we sought to help people recollect, and reexperience their most cherished memories. To accomplish this, we collected a dataset, which integrates multimodal data such as images, text, and corresponding brain signals (EEG) to decode dynamic affect. Our work will be presented at the Tech to Art Platform (TAP) International Conference Prequel.
		  </font>
		  <br>
		</td>
	      </tr> 

	  <!--  SAIT  -->
	      <tr>
		<td style="padding:2%;width:25%;vertical-align:middle">
		  <img src='images/sait.jpg' width="100%" height="auto">
		</td>
		<td style="padding:2%;width:75%;vertical-align:middle">
		  <span style="font-size: 18px"><strong><papertitle>Samsung Advanced Institute of Technology Research Capstone</papertitle></strong></span>
		  <div style="line-height:20%; font size=1"><br></div>
		  <u><strong><span style="font-size: 16px">Sooyoung Kim</span></strong></u>, Joonwoo Kwon
		  <br>
		  <a href="data/sait.pdf">Project slide</a> / <a href="https://github.com/Sooyyoungg/SAIT">Code</a>
		  <br>
		  <div style="line-height:60%; font size=1"><br></div>
		  <font size="2">
		  Semiconductor quality assurance is highly inefficient and costly because its components are too extremely small to inspect. Therefore, we developed an Image-to-Image Translation model that synthesizes 3D depth maps from 2D Scanning Electron Microscope (SEM) images to ensure that semiconductors are produced as intended.
		  </font>
		  <br>
		</td>
	      </tr> 

	   <!--  Mitigating Background Bias  -->
	      <tr>
		<td style="padding:2%;width:25%;vertical-align:middle">
		  <img src='images/background_bias.jpg' width="100%" height="auto">
		</td>
		<td style="padding:2%;width:75%;vertical-align:middle">
		  <span style="font-size: 18px"><strong><papertitle>Mitigating Unwanted Background Biases with Background Data Augmentation</papertitle></strong></span>
		  <div style="line-height:20%; font size=1"><br></div>
		  Jaeheyoung Jeon, <u><strong><span style="font-size: 16px">Sooyoung Kim</span></strong></u>, Jaehwan Lim
		  <br>
		  <a href="data/background_bias.pdf">Paper</a> / <a href="https://github.com/Sooyyoungg/MLVU_data_augmentation">Code</a>
		  <br>
		  <div style="line-height:60%; font size=1"><br></div>
		  <font size="2">
		  We conducted background augmentation techniques using various backgrounds (RGB, black, mean, and human-selected) during training to reduce biases in image classification and object detection.
		  </font>
		  <br>
		</td>
	      </tr> 

	  <!--  Capstone  -->
	      <tr>
		<td style="padding:2%;width:25%;vertical-align:middle">
		  <img src='images/capstone.jpg' width="100%" height="auto">
		</td>
		<td style="padding:2%;width:75%;vertical-align:middle">
		  <span style="font-size: 18px"><strong><papertitle>A Real-Time Face Detecting AI Surveillance Camera</papertitle></strong></span>
		  <div style="line-height:20%; font size=1"><br></div>
		  <u><strong><span style="font-size: 16px">Sooyoung Kim</span></strong></u>, Heajin Lee, Suyeon Kim
		  <br>
		  <a href="https://docs.google.com/presentation/d/1C09tdaitmpU3AXYkY608mx4C1ddftSKv/edit?usp=sharing&ouid=105194653391698211161&rtpof=true&sd=true">Presentation slide</a> / <a href="https://github.com/SuyeonKim1702/AI_CCTV">Code1</a> / <a href="https://github.com/leeheajin/AI_CCTV_2">Code2</a> / <a href="data/ai_cctv.pdf">Poster</a> / <a href="data/ai_cctv_video.mp4">Video</a>
		  <br>
		  <div style="line-height:60%; font size=1"><br></div>
		  <font size="2">
		  We developed a smartphone application that identifies the faces of individuals in front of a household and notifies users of the presence of unknown persons in real-time¬†via a security camera affixed to the door.
		  </font>
		</td>
	      </tr> 

		
        </tbody></table>
	</div>


	<!-- ========== Honors & Awards ========== -->
	<div class="docs-section" id="honor and awards">
	<div style="line-height:160%;">
	  <strong><font size="5"><h5>Honors & Awards üèÜ</h5></font></strong>
	  <div class="row no-gutters">
	    <div class="col-12 col-md-12 text-md-left">
	      <li><b>Grand Prize at AI x Art Hackathon</b>, Oct 2024</li>
	    </div>
	  </div>
	  <div class="row no-gutters">
	    <div class="col-12 col-md-12 text-md-left">
	      <li><b>BrainKorea21 Four Scholarship</b>, 2021‚Äì2022</li>
	    </div>
	  </div>
	  <div class="row no-gutters">
	    <div class="col-12 col-md-12 text-md-left">
	      <li><b>2020 4th Seoul Innovation Challenge</b>, Jan 2020 ‚Äì Sep 2020</li>
	    </div>
	  </div>
	  <div class="row no-gutters">
	    <div class="col-12 col-md-12 text-md-left">
	      <li><b>The 9th Ewha Festival for Business Plan</b>, Mar 2019 ‚Äì Dec 2019</li>
	    </div>
	  </div>
	  <div class="row no-gutters">
	    <div class="col-12 col-md-12 text-md-left">
	      <li><b>EWHA Scholarship</b>, 2018-2020</li>
	    </div>
	  </div>
	  <div class="row no-gutters">
	    <div class="col-12 col-md-12 text-md-left">
	      <li><b>EWHA Merit-Based Scholarship</b> (full tuition) - Awarded to the <b>top 10%</b> of students upon admission, 2017</li>
	    </div>
	  </div>
	  <br>
	</div>
	</div>

	      

	<!-- ========== About Me ========== -->
	<div class="docs-section" id="about me">
	<strong><font size="5"><h5>About Me üé®</h5></font></strong>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	      <tr>
		<td style="padding:2%;width:25%;vertical-align:middle">
		  <img src='images/about_myself.jpg' width="100%" height="auto">
		</td>
		<td style="padding:2%;width:75%;vertical-align:middle">
		  I like swimming a lot, as my first name "ÏàòÏòÅ; Sooyoung" sounds the same as "swimming" in Korean.<br><br>
	      	  I also love movies and dramas with dynamic stories, I am especially a big fan of "The Good Place", a TV series about the afterlife with a philosophical and comedic twist, and "Brooklyn Nine-Nine", an American comedy series about detectives who police the NYPD's 99th Precinct. I really love both discussing movie interpretations and imagining alternative movie endings with other people.
		</td>
	      </tr> 

        </tbody></table>
	</div>
		
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Last updated on April 21, 2025
		  <br>
		  Referred to template taken from <a href="https://jonbarron.info/">here</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
	      
        </td>
      </tr>
    </table>

    </tbody>
  </table>

</body>

</html>
